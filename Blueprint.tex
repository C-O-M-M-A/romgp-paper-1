\documentclass[a4paper, margin=1in, reqno]{RAMArticle}
\usepackage[a4paper, margin=1in]{geometry}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\tr}{tr}
\newcommand*{\ev}[1]{\st{E}\left\lbrack{} #1 \right\rbrack}

%opening
\title{Because it can't hurt to write it down}
\author{}

\begin{document}
\maketitle


\section{Notation}
	Scalars (including functionals) are written in normal typeface, compound quantities bold. Matrices are typically uppercase, (column) vectors lowercase. 
	This does not apply to subscripts, particularly multi-indices. With any integer constant \(M\) we associate \(2^{M}\) multi-indices (tuples)
	\begin{equation*}
		\emptyset\subseteq\vr{m}\subseteq\vr{M}\deq(1,\ldots,M)
	\end{equation*}
	A lowercase multi-index is typically variable or arbitrary, whereas uppercase is fixed constant. 
	Multi-indices apply to parenthesized quantities, and refer to the components occurring in the multi-index, as in
	\begin{equation*}
		\vr{m} = (1,3,4) \quad \Longleftrightarrow \quad (\vr{u})_{\vr{m}} = (u_{1},u_{3},u_{4})
	\end{equation*}
	The modulus of a multi-index is just its cardinality, in this example \(\modulus{\vr{m}}=3\). Normal typeface indicates a singleton multi-index.
	In more than one dimension, multi-indices combine by direct product.
	These multi-index rules apply only to subscripts on parentheses: all other subscripts are just ordinary subscripts. 
	Where parentheses also bear superscript (e.g. for inversion), the multi-index applies first
	\begin{equation*}
		(\Sigma)_{\vr{m}\times\vr{m}}^{-1} \not\equiv (\Sigma^{-1})_{\vr{m}\times\vr{m}}
	\end{equation*}


\section{The Separable Kernel Constraint}
	A Gaussian process has been fit to each term in
	\begin{equation*}
		\vec{\vr{y}} = \vec{\vr{f}}(\vr{X}) + \vec{\vr{e}}
	\end{equation*}
	where
	\begin{align*}
		\dim(\vr{X}) \deq N \times M &\deq (\T{number of samples}) \times (\T{feature/input dimensionality}) \\
		\dim(\vec{\vr{y}}) \deq LN\times 1 &\deq (\T{label/output dimensionality})(\T{number of samples}) \times 1
	\end{align*}
	as the rows of \(L\times N\) output \(\vr{Y}\) are concatenated into the column vector \(\vec{\vr{y}}\). 
	Standardization ensures
	\begin{align*}
		(\vr{0})_\vr{M} = \sum_{n=1}^{N} \vec{\vr{x}}_{n} \deq \sum_{n=1}^{N} (\vr{X})_{n \times \vr{M}}^{\intercal} 
		&\QT{;} (\vr{0})_{\vr{L}} = \sum_{n=1}^{N} (\vec{\vr{y}})_{\vr{L}N+n-N} \\
		N = \sum_{n=1}^{N} \vec{\vr{x}}_{n}^{\intercal}\vec{\vr{x}}_{n} \deq \sum_{n=1}^{N} (\vr{X})_{n \times \vr{M}} (\vr{X})_{n \times \vr{M}}^{\intercal}
		&\QT{;} N (\vr{1})_{\vr{L}} = \sum_{n=1}^{N} (\vec{\vr{y}})^{2}_{\vr{L}N+n-N}	
	\end{align*}
	This enables us to write prior assumptions
	\begin{align*}
		\vec{\vr{e}} &\sim \mathsf{N} [0, \Sigma_{\vr{e}}] \\
		\vec{\vr{f}}(\vr{X}) &\sim \mathsf{N}[0, \Sigma_{\vr{f}} \otimes k(\vr{X},\vr{X})]
	\end{align*}
	of which the most subtle is the separability of \(LN\times LN\) covariance matrix \(\mathsf{E}[\vec{\vr{f}}(\vr{X})\vec{\vr{f}}^{\intercal}(\vr{X})]\) 
	into the Kronecker product of an \(L \times L\) signal covariance \(\Sigma_{\vr{f}}\) and a \textit{single} \(N \times N\) similarity kernel \(k(\vr{X},\vr{X})\).
	This typically appears via the output covariance
	\begin{equation*}
		\Sigma_{\vec{\vr{y}}} \deq \Sigma_{\vr{f}} \otimes k(\vr{X},\vr{X}) + \Sigma_{\vr{e}} \otimes (\vr{I})_{\vr{N}\times\vr{N}}
	\end{equation*}
	where \(\vr{I}\) is the identity matrix.


\section{The Similarity Kernel}
	Without loss of generality, the similarity kernel \( k \colon \st{R}^M \! \times \st{R}^M \to \st{R}\) is required to obey
	\begin{equation*}
		0 \leq k(\vec{\vr{x}}_{n},\vr{x}) = k(\vr{x},\vec{\vr{x}}_{n}) \leq k(\vr{x},\vr{x}) = 1
	\end{equation*}
	for arbitrary \(\vec{\vr{x}}_{n},\vr{x}\). For tractability we \textit{choose} an ARD (Automatic Relevance Determination) kernel
	\begin{equation*}
		k(\vec{\vr{x}}_{n},\vr{x}) \deq 
			(2 \pi)^{M/2} \modulus{\Lambda} \, p\!\left(\vr{x} ; \vec{\vr{x}}_{n}, \Lambda^2\right) 
		\deq \exp\left(-\frac
			{(\vr{x}-\vec{\vr{x}}_{n})^{\intercal}\, \Lambda^{-2} \,(\vr{x}-\vec{\vr{x}}_{n})}
			{2}
		\right) 
	\end{equation*}
	defining \(p\) as the Gaussian probability density, here with \emph{diagonal} positive definite lengthscale matrix \(\Lambda\).
	Let us re-express this in a rotated orthogonal basis
	\begin{equation*}
		\vr{u} \deq \Theta \, \vr{x} 
			\quad \Longleftrightarrow \quad 
			k(\vec{\vr{x}}_{n},\vr{x}) = 
			(2 \pi)^{M/2} \modulus{\Lambda} \, 
			p\!\left(\vr{u} ; \vec{\vr{u}}_{n}, \Theta\Lambda^{2}\Theta^{\intercal}\right)
	\end{equation*}
	where the rows of \(\Theta\) are orthonormal left (eigen)vectors.


\section{Analysis of Variance}
	In this section the singular (\(M\times 1\)) sample datum \(\vr{u}\) is drawn from a standardized normal test distribution
	\begin{equation*}
		\vr{u} \sim \mathsf{N}\lbrack (\vr{0})_{\vr{M}},(\vr{I})_{\vr{M}\times\vr{M}}\rbrack
	\end{equation*}
	The datum elicits the (\(L\times 1\)) response from the emulator
	\begin{equation*}
		\vr{f}(\vr{x}) \deq \vec{\vr{f}}(\vr{x})
		 = \left(\Sigma_{\vr{f}} \otimes k(\vr{X}, \vr{x})\right) \Sigma_{\vec{\vr{y}}}^{-1} \, \vec{\vr{y}}
		 = \vr{\tilde{Y}} \, k(\vr{X}, \vr{x})
	\end{equation*}
	where the (\(L\times N\)) matrix \(\vr{\tilde{Y}}\) is
	\begin{equation*}
		\left(\vr{\tilde{Y}}\right)_{l \times n} \deq \left(\Sigma_{\vr{f}}^{\phantom{1}}\, \Sigma_{\vec{\vr{y}}}^{-1} \, \vec{\vr{y}}\right)_{lN+n-N}
	\end{equation*}

	The task is to optimize the test distribution to capture as much information about \(\vr{f}\) as possible. 
	To this end, feature space \(\st{U} \deq \st{R}^{M}\) is decomposed into the direct product
	\begin{equation*}
		\vr{u} \in \st{U} \cong (\st{U})_{\vr{m}} \times (\st{U})_{\vr{M}\setminus\vr{m}} \ni
		\begin{pmatrix}
			(\vr{u})_{\vr{m}} \\ (\vr{u})_{\vr{M}\setminus\vr{m}}
		\end{pmatrix}
	\end{equation*}
	admitting states of knowledge from the unconditional (\(\vr{m}=\emptyset, (\st{U})_{\vr{m}}=\emptyset\)) 
	to totally conditional (\(\vr{m}=\vr{M},(\st{U})_{\vr{M}\setminus\vr{m}}=\emptyset\)). 
	Decomposition relates to the original by congruence rather than identity to allow index re-ordering, as in \((1,2,3) \cong (1,3)\times (2)\).
	The (independent) test distribution cleaves into
	\begin{equation*}
		\begin{pmatrix}
			(\vr{u})_{\vr{m}} \\ (\vr{u})_{\vr{M}\setminus\vr{m}}
		\end{pmatrix}
		=
		\begin{pmatrix}
		(\vr{u})_{\vr{m}} \,\vert\, (\vr{u})_{\vr{M}\setminus\vr{m}} \\ (\vr{u})_{\vr{M}\setminus\vr{m}} \,\vert\, (\vr{u})_{\vr{m}}
	\end{pmatrix}
		\sim \mathsf{N} \left\lbrack
		\begin{pmatrix}
			(\vr{0})_{\vr{m}} \\ (\vr{0})_{\vr{M}\setminus\vr{m}}
		\end{pmatrix}
		\; , \;
		\begin{pmatrix}
			(\vr{I})_{\vr{m}\times\vr{m}} & 0 \\
			0 & (\vr{I})_{\vr{M}\setminus\vr{m} \times \vr{M}\setminus\vr{m}}
		\end{pmatrix}
		\right\rbrack
	\end{equation*}


	\subsection{Conditional Responses}
		There are (\(2^{M}\)) conditional response expectations
		\begin{equation*}
			\vr{f}_{\vr{m}}((\vr{u})_{\vr{m}}) 
				\deq \mathsf{E}\left[\vr{f}(\vr{x}) \vert (\vr{u})_{\vr{m}} \right] 
			\phantom{:}= \int 
			\vr{\tilde{Y}} \, k(\vr{X}, \vr{x}) \; 
				\frac
					{p\!\left((\vr{u})_{\vr{M}} ; (\vr{0})_{\vr{M}},(\vr{I})_{\vr{M}\times\vr{M}}\right)} 
					{p\!\left((\vr{u})_{\vr{m}} ; (\vr{0})_{\vr{m}},(\vr{I})_{\vr{m}\times\vr{m}}\right)} 
			\, \mathrm{d} (\vr{u})_{\vr{M}\setminus\vr{m}}	
		\end{equation*}
		each an (\(L\times 1\)) vector
		\begin{equation*}
			\vr{f}_{\vr{m}}((\vr{u})_{\vr{m}}) = \vr{F} \; 
				\frac 
					{p^{\intercal}\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{\vr{N}\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)}
					{p\!\left((\vr{u})_{\vr{m}} ; (\vr{0})_{\vr{m}},(\vr{I})_{\vr{m}\times\vr{m}}\right)}
		\end{equation*}
		The (\(L\times N\)) matrix \(\vr{F}\) is
		\begin{equation*}
			\vr{F} \deq \modulus{\Lambda^{-2}+\vr{I}}^{-1/2} \; \sum_{n=1}^{N} \big(\vr{\tilde{Y}}\big)_{\vr{L} \times n} 
				\; \big(\vr{I}\big)_{n \times \vr{N}} \; \exp\!\left(-\frac{\vec{\vr{x}}_{n}^{\intercal}
				\left(\Lambda^{2} + \vr{I}\right)^{-1}\vec{\vr{x}}_{n}}{2}\right)
		\end{equation*}
		which is independent of \(\Theta\) and therefore \(\vr{m}\).
		The (\(N\times 1\)) vector 
		\(p^{\intercal}\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{\vr{N}\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)\) 
		is composed of
		\begin{equation}\label{eq:ANOVA:pT}
			\left(p^{\intercal}\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{\vr{N}\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)\right)_{\!n}
			\deq
			p\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{n\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)
		\end{equation}
		where
		\begin{align*}
			\vr{T} &\deq 
				\vr{X} \left(\Lambda^{2} + \vr{I}\right)^{-1} \Theta^{\intercal} \\
			\Sigma &\deq 
				\Theta \, \left(\Lambda^{-2} + \vr{I}\right)^{-1} \Theta^{\intercal}
		\end{align*}
		The (\(\modulus{\vr{m}}\times 1\)) mean vector in \cref{eq:ANOVA:pT} thus equates to
		\begin{equation*}
			(\vr{T})_{n\times\vr{m}}^{\intercal} \deq
			(\Theta)_{\vr{m}\times\vr{M}} \left(\Lambda^{2} + \vr{I}\right)^{-1} \vec{\vr{x}}_{n}
		\end{equation*}

		According to these formulae, the unconditional response is just
		\begin{equation*}
			\vr{f}_{\emptyset} = \vr{F} \, (\vr{1})_{\vr{N}}
		\end{equation*}
		while the totally conditional response is
		\begin{equation*}
			\vr{f}_{\vr{M}}((\vr{u})_{\vr{M}}) 
			\deq \mathsf{E}\left[\vr{f}(\vr{x}) \vert (\vr{u})_{\vr{M}} \right] = \vr{f}(\vr{u}) \deq \vec{\vr{f}}(\vr{x})\\ 
		\end{equation*}


	\subsection{Conditional Variances}
		There are \(2^{M}\) conditional (co)variances
		\begin{align*}
			\vr{D}_{\vr{m}} &\deq 
				\mathsf{E}\left[
					\mathsf{E}\left[\vr{f}(\vr{x}) \vert (\vr{u})_{\vr{m}} \right]
					\mathsf{E}\left[\vr{f}^{\intercal}(\vr{x}) \vert (\vr{u})_{\vr{m}} \right]
				\right]
				- \mathsf{E}[\vr{f}(\vr{x})] \mathsf{E}[\vr{f}^{\intercal}(\vr{x})] \\
			&\phantom{:}= 
				\mathsf{E}\left[
					\vr{f}_{\vr{m}}((\vr{u})_{\vr{m}})\vr{f}^{\intercal}_{\vr{m}}((\vr{u})_{\vr{m}})
				\right]
				- \vr{f}_{\emptyset}\vr{f}^{\intercal}_{\emptyset}
		\end{align*}
		each an (\(L\times L\)) matrix of components
		\begin{equation*}
			\vr{D}_{\vr{m}} = 
			\frac
				{\vr{F} \; \vr{W}_{\vr{m}} \; \vr{F}^{\intercal}}
				{\modulus{2(\Sigma)_{\vr{m}\times\vr{m}} - (\Sigma)_{\vr{m}\times\vr{m}}^{2}}^{1/2}}
			- \vr{F}\; \vr{1} \;\vr{F}^{\intercal} 
		\end{equation*}
		such that
		\begin{equation}\label{eq:ANOVA:qformula}
			\frac
				{\vr{W}_{\vr{m}}}
				{\modulus{2(\Sigma)_{\vr{m}\times\vr{m}} - (\Sigma)_{\vr{m}\times\vr{m}}^{2}}^{1/2}}
			=
				\int
				\frac{
					p^{\intercal}\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{\vr{N}\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)
					\; p\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{\vr{N}\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)}
					{p\!\left((\vr{u})_{\vr{m}} ; (\vr{0})_{\vr{m}},(\vr{I})_{\vr{m}\times\vr{m}}\right)}
		\, \mathrm{d} (\vr{u})_{\vr{m}}
		\end{equation}
		This is achieved by (\(N\times N\)) symmetric \(\vr{W}_{\vr{m}}\) of the form
		\begin{equation*}
			\begin{aligned}
				(\vr{W}_{\vr{m}})_{n \times o} &\deq
				\exp\!\left(\frac{
					-(\vr{T})_{n\times\vr{m}}(\Sigma)_{\vr{m}\times\vr{m}}^{-1}
					(\vr{T})_{n\times\vr{m}}^{\intercal} 
					-(\vr{T})_{o\times\vr{m}}(\Sigma)_{\vr{m}\times\vr{m}}^{-1}
					(\vr{T})_{o\times\vr{m}}^{\intercal}
					}{2}\right) \\
					&\phantom{deq} \exp\!\left(\frac{
						+ \left((\vr{T})_{n\times\vr{m}}+(\vr{T})_{o\times\vr{m}}\right)
						(\Phi)_{\vr{m}\times\vr{m}}^{-1}(\Sigma)_{\vr{m}\times\vr{m}}^{-1}
						\left((\vr{T})_{n\times\vr{m}}^{\intercal}+(\vr{T})_{o\times\vr{m}}^{\intercal}\right)
						}{2}\right)
			\end{aligned}
		\end{equation*}
		where
		\begin{equation}\label{eq:ANOVA:qdef}
			\Phi \deq \Theta \left(\Lambda^{-2}+\vr{I}\right)^{-1}\left(2\Lambda^{-2}+\vr{I}\right) 
			\Theta^{\intercal}
		\end{equation}
		
		The unconditional variance is, by definition, simply
		\begin{equation*}
			\vr{D}_{\emptyset} \deq \mathsf{E}\left[
			\vr{f}_{\emptyset}\vr{f}^{\intercal}_{\emptyset}
			\right]
			- \vr{f}_{\emptyset}\vr{f}^{\intercal}_{\emptyset}
			= 0
		\end{equation*}
		The totally conditional (or total) variance \(\vr{D}_{\vr{M}}\) is entirely independent of \(\Theta\) because \((\Theta)_{\vr{M}\times\vr{M}}=\Theta\) is orthogonal.	


\section{Sobol' Indices and Their Derivatives}
	The (\(L\times L\)) Sobol' index associated with multi-index \(\vr{m}\) is just the fraction of 
	total variance attributable to the feature subspace \(\st{U}_{\vr{m}}\)
	\begin{equation*}
		\vr{S}_{\vr{m}} \deq \vr{D}_{\vr{M}}^{-\circ}\circ\vr{D}_{\vr{m}}
	\end{equation*}
	where \(\circ\) is the Hadamard (element-wise) product and \(\vr{D}_{\vr{M}}^{-\circ}\) is the Hadamard inverse 
	(element-wise reciprocal) obeying \(\vr{D}_{\vr{M}}^{-\circ}\vr{D}_{\vr{M}} = \vr{1}\).
	Note that
	\begin{equation*}
		\vr{S}_{\vr{M}\setminus\vr{m}} \leq \vr{1} - \vr{S}_{\vr{m}} \QT{element-wise}
	\end{equation*}
	because the left is variance totally independent of \(\st{U}_{\vr{m}}\), whereas the right is variance which is not due to \(\st{U}_{\vr{m}}\) alone.

	The space to be optimized \(\st{U}_{\vr{m}}\) is precisely the linear span of the (row) vectors comprising \((\Theta)_{\vr{m} \times \vr{M}}\). Let all quantities be fixed save 
	\(\Theta = \Theta(\alpha)\), then isolating terms free of \(\Theta\) yields
	\begin{equation*}
		\frac{\partial\,\vr{S}_{\vr{m}}}{\partial\,\alpha}
		= \vr{D}_{\vr{M}}^{-\circ} \; 
			\frac{\partial\left(\vr{D}_{\vr{m}} + \vr{F}\; \vr{1} \;\vr{F}^{\intercal}\right)}
				{\partial\,\alpha}
		= \vr{D}_{\vr{M}}^{-\circ} \; \frac{
				\vr{F}\; \left(\vr{W}_{\vr{m}} \circ \vr{V}\right)\; \vr{F}^{\intercal}
				- \vr{F}\; \vr{W}_{\vr{m}}\; D \; \vr{F}^{\intercal}}
			{\modulus{2(\Sigma)_{\vr{m}\times\vr{m}} - (\Sigma)_{\vr{m}\times\vr{m}}^{2}}^{1/2}}
		\end{equation*}
	where
	\begin{align*}
 		(\vr{V})_{n \times o} &\deq
			\frac{\partial\,\log\,(\vr{W}_{\vr{m}})_{n \times o}} {\partial\,\alpha} \\
		D &\deq
			\frac{\partial}{\partial\,\alpha}
				\log\modulus{2(\Sigma)_{\vr{m}\times\vr{m}} - (\Sigma)_{\vr{m}\times\vr{m}}^{2}}^{1/2}
 	\end{align*}
	These vanish whenever
	\begin{equation*}
		\frac{\partial\,(\Theta)_{\vr{m}\times\vr{M}}}{\partial\,\alpha} = 0
		\quad \Longrightarrow \quad \vr{V} = D = 0
	\end{equation*}
	otherwise they may be evaluated element-wise.

\section{Optimization Strategy}
The optimization strategy we prefer is to maximize \(\vr{S}_{\vr{m}}\) for retained features \(\st{U}_{\vr{m}}\).
This equates to maximizing retained variance, which is preferable to minimizing variance solely due to rejected features.
The optimization target will be \(\lVert\vr{S}_{\vr{m}}\rVert\) for some semi-norm \(\lVert\cdot\rVert\) on the space of symmetric \(L\times L\) matrices. 
A semi-norm is used primarily because we may wish to ignore some output dimensions when optimizing.

The efficient way to maximize \(\vr{S}_{\vr{m}}\) is to take \(\vr{m} = \set{l < m}\) for \(m=1,2,3,\ldots\) in turn.
Sequential updates at each \(m\) are expressed by decomposing to
\begin{equation*}
	(\Theta)_{\vr{m}\cup\set{m}\times\vr{M}} = (\Xi)_{\vr{m}\cup\set{m}\times\vr{M}} \; \hat{\Theta}
\end{equation*}
where \(\Xi\) is orthogonal, and identical on the subspace \((\st{U})_{\vr{m}}\) already optimized
\begin{align*}
	(\Xi)_{\vr{m}\times\vr{M}} = \vr{I}_{\vr{m}\times\vr{M}} \quad \text{and} \quad
	(\Xi)_{m\times\vr{m}} = (\vr{0})_{1\times\vr{m}}
\end{align*}
The idea is to optimize row $m$ of $\Xi$, then update $\hat{\Theta}$ such that $(\Xi)_{\vr{m}\cup\set{m}\times\vr{M}} = \vr{I}_{\vr{m}\cup\set{m}\times\vr{M}}$. This is easily achieved by
\begin{equation*}
	\hat{\Theta} = \vr{Q}^{\intercal} \QT{where} \Theta^{\intercal} = \vr{Q}\vr{R} \QT{is the QR factorization of the update.}
\end{equation*}
Row $m$ of $\Xi$ has at most $(M-m)$ non-zero entries to maintain orthogonality with 
$(\Xi)_{\vr{m}\times\vr{M}}$, and the last of these is fixed by normalisation:
\begin{equation*}
	(\Xi)_{m\times M} = \left(1 - \sum_{k=m}^{M-1} (\Xi)_{m\times k}^{2} \right)^{1/2}
\end{equation*}
This introduces a derivative adjustment
\begin{equation*}
	\frac{\partial}{\partial\,(\Xi)_{m\times k}} = \frac{\partial}{\partial\,(\Xi)_{m\times k}} - 
	\frac{(\Xi)_{m \times k}}{(\Xi)_{m \times M}}\frac{\partial}{\partial\,(\Xi)_{m\times M}}
\end{equation*}
The sequential update \((\Xi)_{m\times\vr{M}\setminus\vr{m}}\) induces
\begin{equation*}
	\vr{S}_{\vr{m}\cup\set{m}}
	= \vr{D}_{\vr{M}}^{-\circ} \; 
	\frac{\vr{F}\; \vr{W}_{\vr{m}\cup\set{m}}\; \vr{F}^{\intercal}}
	{\modulus{2(\Sigma)_{\vr{m}\cup\set{m}\times\vr{m}\cup\set{m}} - 
	(\Sigma)_{\vr{m}\cup\set{m}\times\vr{m}\cup\set{m}}^{2}}^{1/2}}
	- \vr{D}_{\vr{M}}^{-\circ} \; \vr{F}\; \vr{1} \; \vr{F}^{\intercal}
\end{equation*}

\section{Bollocks}
via
\begin{gather*}
	{\modulus{(\Xi)_{\vr{m}\cup\set{m}\times\vr{M}}\left(\Theta\Delta\Theta^{\intercal}\right)(\Xi^{\intercal})_{\vr{M}\times\vr{m}\cup\set{m}}}} \phantom{:}= 
	\modulus{\left(\Theta\Delta\Theta^{\intercal}\right)_{\vr{m}\times\vr{m}}}
	\modulus{(\Xi)_{m\times\vr{M}\setminus\vr{m}}\, \vr{E} \, (\Xi^{\intercal})_{\vr{M}\setminus\vr{m}\times m}} \\
	\vr{E} \deq 
		\left(\Theta\Delta\Theta^{\intercal}\right)_{\vr{M}\setminus\vr{m}\times\vr{M}\setminus\vr{m}}
		- (\Delta)_{\vr{M}\setminus\vr{m}\times\vr{M}\setminus\vr{m}}
		(\Theta^{\intercal})_{\vr{M}\setminus\vr{m}\times\vr{m}}
		(\Theta)_{\vr{m}\times\vr{M}\setminus\vr{m}}
\end{gather*}
and
\begin{align*}
	(\vr{v}_n)_{m}
		&= \sum_{k=m}^{M} (\Xi)_{m\times k} (\vr{v}_n)_{k} \\
	(\vr{w}_n+\vr{w}_o)_{m}
		&= \sum_{k=m}^{M} (\Xi)_{m\times k} (\vr{w}_n+\vr{w}_o)_{k} 
\end{align*}
The sensitivities are
\begin{equation*}
	\frac{\vr{S}_{\vr{m}\cup\set{m}}}{\partial\,(\Xi)_{m \times k}}
	 = \vr{D}_{\vr{M}}^{-\circ} \; \frac{
	 	\vr{F}\; \left(\vr{W}_{\vr{m}\cup\set{m}} \circ \vr{V}_{mk}\right)\; \vr{F}^{\intercal}
	 	- \vr{F}\; \vr{W}_{\vr{m}}\; D_{mk} \; \vr{F}^{\intercal}}
	 	{\modulus{(\Xi)_{\vr{m}\cup\set{m}\times\vr{M}}\left(\Theta\Delta\Theta^{\intercal}\right)(\Xi^{\intercal})_{\vr{M}\times\vr{m}\cup\set{m}}}}
	\end{equation*}
where
\begin{align*}
	(\vr{V}_{mk})_{n \times o} 
		&= - (\vr{v}_{n})_{m}\frac{\partial\,(\vr{v}_{n})_{m}} {\partial\,(\Xi)_{m \times k}} 
			+ (\vr{w}_{n} + \vr{w}_{o})_{m}
			\frac{\partial\,(\vr{w}_{n} + \vr{w}_{o})_{m}} {\partial\,(\Xi)_{m \times k}} 
			-(\vr{v}_{o})_{m}\frac{\partial\,(\vr{v}_{o})_{m}} {\partial\,(\Xi)_{m \times k}} \\
		&=	- (\vr{v}_{n})_{m}(\vr{v}_{n})_{k} 
			+(\vr{w}_{n} + \vr{w}_{o})_{m}(\vr{w}_{n} + \vr{w}_{o})_{k}
			- (\vr{v}_{o})_{m}(\vr{v}_{o})_{k} 
\end{align*}
\begin{align*}
	D_{mk} &=
		\frac{1}{2}\;\tr\!\left(
			\left(
				(\Xi)_{\vr{m}\cup\set{m}\times\vr{M}}\left(\Theta\Delta\Theta^{\intercal}\right)(\Xi^{\intercal})_{\vr{M}\times\vr{m}\cup\set{m}}
			\right)^{-1}
			\; \frac{\partial
			\left(
				(\Xi)_{\vr{m}\cup\set{m}\times\vr{M}}\left(\Theta\Delta\Theta^{\intercal}\right)(\Xi^{\intercal})_{\vr{M}\times\vr{m}\cup\set{m}}
			\right)}
			{\partial\,(\Xi)_{m \times k}}
		\right) \\
		&= 
		\sum_{m\in\vr{m}} \, (\Delta)_{k \times k} \, (\Theta)_{m \times k}
		\sum_{i=1}^{M} (\Theta)_{m \times i} \, (\Delta)_{i \times i}^{-1} \, (\Theta)_{m \times i}
\end{align*}
for diagonal
\begin{equation*}
\end{equation*}

\section{Appendix}
	The conditional response expectations and covariances derive from the following rules of evaluation.
	Full generality is not sought: it suffices to consider symmetric positive definite matrices \(\vr{A},\vr{B},\vr{C}\), arbitrary vectors \(\vr{A},\vr{B},\vr{C}\), diagonal positive definite \(\Lambda\) and orthogonal \(\Theta\).

	\subsection{Symmetric matrix multiplication}\label{sub:App:SMM}
		The product of two symmetric matrices commutes.

	\subsection{Gaussian product rule}\label{sub:App:GPR} 
		See \cite{Roweis1999,Rasmussen2016,Schoen.Lindsten2018}.
		\begin{align*}
			p\!\left((\vr{u})_{\vr{m}} ; (\vr{a})_{\vr{m}},(\vr{A})_{\vr{m}\times\vr{m}}\right) &
			\; p\!\left((\vr{u})_{\vr{m}} ; (\vr{b})_{\vr{m}},(\vr{B})_{\vr{m}\times\vr{m}}\right) \\
			&= (2 \pi)^{-\modulus{\vr{m}}/2}
			\Big\lvert(\vr{A})_{\vr{m}\times\vr{m}} + (\vr{B})_{\vr{m}\times\vr{m}}\Big\rvert^{-1/2}
				\exp\left( -\frac{d}{2}\right) p(\vr{u} ; \vr{c},\vr{C}) \\
			&= (2 \pi)^{-\modulus{\vr{m}}/2}\modulus{(\vr{A})_{\vr{m}\times\vr{m}}^{-1/2}} 
				\modulus{(\vr{B})_{\vr{m}\times\vr{m}}^{-1/2}}
				\modulus{(\vr{C})_{\vr{m}\times\vr{m}}^{1/2}}
				\exp\left( -\frac{d}{2} \right) p(\vr{u} ; \vr{c},\vr{C})
		\end{align*}
		where
		\begin{align*}
			\vr{C} &\deq \left((\vr{A})_{\vr{m}\times\vr{m}}^{-1} + (\vr{B})_{\vr{m}\times\vr{m}}^{-1}\right)^{-1} \\
			\vr{c} &\deq \vr{C}\left((\vr{A})_{\vr{m}\times\vr{m}}^{-1} (\vr{a})_{\vr{m}} 
				+ (\vr{B})_{\vr{m}\times\vr{m}}^{-1} (\vr{b})_{\vr{m}}\right) \\
			d &\deq \left((\vr{b})_{\vr{m}}-(\vr{a})_{\vr{m}}\right)^{\intercal} \left((\vr{A})_{\vr{m}\times\vr{m}} + (\vr{B})_{\vr{m}\times\vr{m}}\right)^{-1} \left((\vr{b})_{\vr{m}}-(\vr{a})_{\vr{m}}\right) \\
			&\phantom{:}= 
				(\vr{a})_{\vr{m}}^{\intercal} (\vr{A})_{\vr{m}\times\vr{m}}^{-1} (\vr{a})_{\vr{m}}
				+ (\vr{b})_{\vr{m}}^{\intercal} (\vr{B})_{\vr{m}\times\vr{m}}^{-1} (\vr{b})_{\vr{m}}
				- (\vr{c})_{\vr{m}}^{\intercal} (\vr{C})_{\vr{m}\times\vr{m}}^{-1} (\vr{c})_{\vr{m}}
		\end{align*}

	\subsection{Gaussian marginalisation}\label{sub:App:GM}
		See \cite{Roweis1999,Rasmussen2016,Schoen.Lindsten2018}.
		Provided \((\vr{u})_{\vr{m}}\) and \((\vr{u})_{\vr{M}\setminus\vr{m}}\) are independent random variables
		\begin{equation*}
			\int p\!\left( \vr{u} ; \vr{a},\vr{A} \right) \, \mathrm{d} (\vr{u})_{\vr{M}\setminus\vr{m}}
				= p\!\left((\vr{u})_{\vr{m}} ; (\vr{a})_{\vr{m}},(\vr{A})_{\vr{m}\times\vr{m}}\right)
		\end{equation*}

	\subsection{Product Marginalization}\label{sub:App:PM}
		As one would hope, marginalization generally eliminates the corresponding row vectors from \(\Theta\) and column vectors from \(\Theta^{\intercal}\).
		\begin{align*}
			(\Theta\vr{a})_{\vr{m}} &= 
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{a} \\
			(\Theta\vr{A}\Theta^{\intercal})_{\vr{m}\times\vr{m}} &= 
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{A}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}} \\
			(\Theta)_{\vr{m}\times\vr{M}}^{\intercal} &= 
			(\Theta^{\intercal})_{\vr{M}\times\vr{m}}
		\end{align*}

	\subsection{Orthogonal Inner and Outer Products}\label{sub:App:OIOP}
		The inner square of a marginalized orthogonal matrix is identity.
		\begin{equation*}
			(\hat{\Theta})_{\vr{m}\times\vr{M}}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}} = 
			(\vr{I})_{\vr{m}\times\vr{m}}
		\end{equation*}
		The outer square is also symmetric because
		\begin{equation*}
			\left((\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,(\Theta)_{\vr{m}\times\vr{M}}\right)_{k \times l} = 
			\sum_{m\in\vr{m}} (\Theta)_{m \times k}(\Theta)_{m \times l} = 
			\left((\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,(\Theta)_{\vr{m}\times\vr{M}}\right)_{l \times k}
		\end{equation*}
		and generally obeys
		\begin{equation*}
			(\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,(\Theta)_{\vr{m}\times\vr{M}} = 
			(\vr{I})_{\vr{M}\times\vr{M}} -
			(\Theta^{\intercal})_{\vr{M}\times\vr{M}\setminus\vr{m}}\,(\Theta)_{\vr{M}\setminus\vr{m}\times\vr{M}}
		\end{equation*}

	\subsection{Multiplication of Marginalized Products}\label{sub:App:MMP}
		\begin{align*}
			(\Theta\vr{A}\Theta^{\intercal})_{\vr{m}\times\vr{m}}\,&(\Theta\vr{B}\Theta^{\intercal})_{\vr{m}\times\vr{m}}
			\,(\Theta\vr{C}\Theta^{\intercal})_{\vr{m}\times\vr{m}}\\
			&= 
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{A}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{B}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{C}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}} \\
			&= 
			(\Theta)_{\vr{m}\times\vr{M}}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{A}\vr{B}\vr{C}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,
			(\Theta)_{\vr{m}\times\vr{M}}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}} \\
			&= 
			(\Theta)_{\vr{m}\times\vr{M}}\,\vr{A}\vr{B}\vr{C}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}\,
		\end{align*}

	\subsection{Inversion of Marginalized Products}\label{sub:App:IMP}
		Being symmetric positive definite, \(\vr{A}\) is invertible, and
		\begin{equation*}
			(\Theta\vr{A}^{-1}\Theta^{\intercal})_{\vr{m}\times\vr{m}}\,(\Theta\vr{A}\Theta^{\intercal})_{\vr{m}\times\vr{m}}
			= (\Theta)_{\vr{m}\times\vr{M}}\,\vr{A}^{-1}\vr{A}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}
			= (\vr{I})_{\vr{m}\times\vr{m}}
		\end{equation*}
		from \cref{sub:App:MMP}, so
		\begin{equation*}
			(\Theta\vr{A}\Theta^{\intercal})_{\vr{m}\times\vr{m}}^{-1}
			= (\Theta\vr{A}^{-1}\Theta^{\intercal})_{\vr{m}\times\vr{m}}
			= (\Theta)_{\vr{m}\times\vr{M}}\,\vr{A}^{-1}\,(\Theta^{\intercal})_{\vr{M}\times\vr{m}}
		\end{equation*}
	
	\subsection{Variance Evaluation}
		Completing the square in \cref{eq:ANOVA:qformula} using \cref{sub:App:GPR} yields an exponent
		\begin{align*}
			(q_\vr{m})_{n\times o}
			&\deq (\vr{T})_{n\times\vr{m}} (\Sigma)_{\vr{m}\times\vr{m}}^{-1} (\vr{T})_{n\times\vr{m}}^{\intercal}
			+ (\vr{T})_{o\times\vr{m}} (\Sigma)_{\vr{m}\times\vr{m}}^{-1} (\vr{T})_{o\times\vr{m}}^{\intercal} - 
			\vr{r}_{no\vr{m}}^{\intercal} \left(2(\Sigma)_{\vr{m} \times \vr{m}}^{-1} - (\vr{I})_{\vr{m} \times \vr{m}}\right) \vr{r}_{no\vr{m}} \\
			\vr{r}_{no\vr{m}}
			&\deq 
			\left(2(\Sigma)_{\vr{m} \times \vr{m}}^{-1} - (\vr{I})_{\vr{m} \times \vr{m}}\right)^{-1}(\Sigma)_{\vr{m} \times \vr{m}}^{-1} 
			\left(((\vr{T})_{n\times\vr{m}}^{\intercal} + (\vr{T})_{o\times\vr{m}}^{\intercal}\right)
		\end{align*}
		Applying \crefrange{sub:App:PM}{sub:App:IMP} to these expressions yields the result used, \cref{eq:ANOVA:qdef}.

\bibliography{main}
\bibliographystyle{abbrvnat}

\end{document}
