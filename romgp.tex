%% 
    %% Copyright 2007-2018 Elsevier Ltd
    %% 
    %% This file is part of the 'Elsarticle Bundle'.
    %% ---------------------------------------------
    %% 
    %% It may be distributed under the conditions of the LaTeX Project Public
    %% License, either version 1.2 of this license or (at your option) any
    %% later version.  The latest version of this license is in
    %%    http://www.latex-project.org/lppl.txt
    %% and version 1.2 or later is part of all distributions of LaTeX
    %% version 1999/12/01 or later.
    %% 
    %% The list of all files belonging to the 'Elsarticle Bundle' is
    %% given in the file `manifest.txt'.
    %% 

    %% Template article for Elsevier's document class `elsarticle'
    %% with numbered style bibliographic references
    %% SP 2008/03/01
    %%
    %% 
    %%
    %% $Id: elsarticle-template-num.tex 64 2013-05-15 12:23:51Z rishi $
    %%
    %%
\documentclass[preprint,12pt]{elsarticle}

    %% Use the option review to obtain double line spacing
    %% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

    %% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
    %% for a journal layout:
    %% \documentclass[final,1p,times]{elsarticle}
    %% \documentclass[final,1p,times,twocolumn]{elsarticle}
    %% \documentclass[final,3p,times]{elsarticle}
    %% \documentclass[final,3p,times,twocolumn]{elsarticle}
    %% \documentclass[final,5p,times]{elsarticle}
    %% \documentclass[final,5p,times,twocolumn]{elsarticle}

    %% For including figures, graphicx.sty has been loaded in
    %% elsarticle.cls. If you prefer to use the old commands
    %% please give \usepackage{epsfig}

    %% The amsthm package provides extended theorem environments
    %% \usepackage{amsthm}

    %% The lineno packages adds line numbers. Start line numbering with
    %% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
    %% for the whole article with \linenumbers.
    %% \usepackage{lineno}

%%%%%%%%%% Robert's macros %%%%%%%%%%%%
    \usepackage{algorithm}
    \usepackage{algorithmic}

    %% The amssymb package provides various useful mathematical symbols
    \usepackage{amssymb}
    \usepackage{amsmath}
    \usepackage[capitalize]{cleveref}

    \usepackage{xspace} 
    \usepackage{ifthen} 
    \usepackage{csvsimple}
    \usepackage{todonotes}

    \newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
    \newcommand*{\vr}[1]{\M{\mathbf{#1}}} 
    \newcommand*{\st}[1]{\M{\mathbb{#1}}} 
    \newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
    \newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
    \newcommand{\T}[1]{\text{#1}} 
    \newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
    \newcommand*{\ev}[2][]{\mathsf{E}_{#1}\!\left\lbrack{} #2 \right\rbrack}
    \newcommand*{\var}[2][]{\mathsf{Var}_{#1}\!\left\lbrack{} #2 \right\rbrack}
    \newcommand*{\gauss}[2]{\mathsf{N}\!\left\lbrack{} #1 , #2 \right\rbrack}
    \newcommand*{\modulus}[1]{\M{\left\lvert#1\right\rvert}} 
    \newcommand*{\norm}[1]{\M{\left\lVert#1\right\rVert}} 
    \newcommand*{\set}[1]{\M{\left\lbrace#1\right\rbrace}} 
    \newcommand*{\setbuilder}[2]{\M{\left\lbrace#1 \: \big\vert \: #2\right\rbrace}}
    \DeclareMathOperator*{\argmax}{argmax}

\journal{Reliability Engineering and System Safety}

\begin{document}


    \begin{frontmatter}

        %% Title, authors and addresses

        %% use the tnoteref command within \title for footnotes;
        %% use the tnotetext command for theassociated footnote;
        %% use the fnref command within \author or \address for footnotes;
        %% use the fntext command for theassociated footnote;
        %% use the corref command within \author for corresponding author footnotes;
        %% use the cortext command for theassociated footnote;
        %% use the ead command for the email address,
        %% and the form \ead[url] for the home page:
        %% \title{Title\tnoteref{label1}}
        %% \tnotetext[label1]{}
        %% \author{Name\corref{cor1}\fnref{label2}}
        %% \ead{email address}
        %% \ead[url]{home page}
        %% \fntext[label2]{}
        %% \cortext[cor1]{}
        %% \address{Address\fnref{label3}}
        %% \fntext[label3]{}

        \title{Minimum  Reduced Order Modelling}

        %% use optional labels to link authors explicitly to addresses:
        %% \author[label1,label2]{}
        %% \address[label1]{}
        %% \address[label2]{}
        
        \author{Robert A. Milton}
        \ead{r.a.milton@sheffield.ac.uk}

        \author{Solomon F. Brown}
        \ead{s.f.brown@sheffield.ac.uk}

        \address{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom}       

        \begin{abstract}
            %% Text of abstract

        \end{abstract}

        \begin{keyword}
            Gaussian Process, Global Sensitivity Analysis, Sobol' Index, Surrogate Model

            %% PACS codes here, in the form: \PACS code \sep code

            %% MSC codes here, in the form: \MSC code \sep code
            %% or \MSC[2008] code \sep code (2000 is the default)

        \end{keyword}

    \end{frontmatter}

    %% \linenumbers

    %% main text
    \section{Introduction} \label{sec:Intro}
        The simulation of a large range of engineering systems requires the application of complex computational models. The use of these models often requires considerable computational effort, and can be prohibitive when attempting to use the underlying model as part of a system optimization. As these models, ultimately, provide the response of some noisy scalar quantity $y(\vr{x})$ to its \M{M}-dimensional input $\vr{x}$; it would therefore be extremely desirable to reduce \M{M} as far as possible using a surrogate or emulator without materially impacting the fidelity of the prediction of \M{y} (reference Sacks). 
        
        One popular method with which to construct such surrogates models is with Gaussian Processes (GPs)(references), given that they are non-parametric, flexible, efficient and analytically tractable (references). Other, parametric, methods include Polynomial Chaos Expansions or POD,... . These, however, are less flexible...
        
        The use of all techniques for the development of surrogate models requires an exponentially growing number of outputs results $y(\vr{x})$ throughout the input space as \M{M} increases: known as the curse of dimensionality. There is therefore a significant driver for methods with which to reduce the dimensionality of the input space, and so a more efficient means of generating the emulator. One way of selecting the directions of most influence is through the application of Global Sensitivity Analysis, however where these directions are not aligned with the input basis this resulting dimensionality reduction is sub-optimal (references).
        
        As such a number of approaches to obtaining the optimal dimensionality reduction have been developed (references), which can broadly be catagorized through their use of different sensitivity measures. For example, \cite{Constantine2014} proposed a means of calculating this optimal reduced dimensional space, the Active Subspace, through a derivative sensitivity measure. This was found to work very effectively for a broad range of problems but requires the information of the derivative which is typically not available for very large or complex systems. \cite{Liu2017} suggested using...
        
        Minimum Average Variance Estimation (MAVE), such as that  proposed by (reference), on the other hand uses variance-based sensitivity measures 
         
A key variance-based sensitivity measure are the Sobol' indices \cite{Sobol2001}, which is one of the most widely used approaches for GSA. With the increased popularity of using GPs as surrogate models, the main disadvantage to the Sobol' method was solved \cite{Oakley2004,Jin2004,Marrel2009}. The use of GPs provided an alternative method of estimating multidimensional integrals using Monte Carlo schemes. This alternative method was incredibly useful as the previous required a large number of model evaluations, nearly 10'000 are needed to reach 10\% precision \cite{Lamoureux2014}. GPs are an incredibly useful surrogate modelling technique for sensitivity analysis as they have the great advantage of being analytically tractable and so can calculate the sensitivity indices given a training set of model evaluations.  The derivation enabling the use of GP regression for analytical evaluation of variance-based sensitivity indices were first introduced by Jin et al. \cite{Jin2004} who applied the Sobol' index formula directly to the GP predictors. Before Oakley and O'Hagan \cite{Oakley2004} used the global stochastic model of a GP, providing the calculations to produce random variables as a new sensitivity measures. Oakley and O'Hagan's \cite{Oakley2004} model allows the sensitivity indices accuracy to be analysed due to the distribution of the variables. Marrel et al. \cite{Marrel2009} extended this comparing them and building on the work from Oakley and O'Hagan \cite{Oakley2004} leading to a novel algorithm which builds confidence intervals for the Sobol' indices. Marrel et al. \cite{Marrel2009} tested both methods on toy functions providing results that show very accurate sensitivity indices and satisfactory confidence intervals from the second method. However, when the approach was illustrated on real data to provide a sensitivity analysis on radionuclide groundwater transport, it was found that the confidence intervals were inaccurate for very low indices due to overestimation of the lowest Sobol' indices.

The purpose of this work is to present a Global Sensitivity Analysis based model order reduction approach, which uses 
\begin{itemize}
\item One point; 
\item second point.
\end{itemize}

This paper is organised as follows: \cref{sec:TheoryReview} presents a review of the concepts and measures that are used in this work. \cref{sec:Method} describes the approach developed, including the details of the calculation of the Sobol' indices and of the optimization of the basis of the optimal low dimensional subspace. \cref{sec:Results} presents the application of the method to a variety of test problems to assess its performance, while \cref{sec:Conclusions} summarises our findings and directions for future work. 

\section{Review of Gaussian Processes and Global Sensitivity Analysis} \label{sec:TheoryReview}
   \subsection{Gaussian Process Surrogate}
            In order to avoid the difficulty and expense in obtaining and analyzing response data from a computationally heavy model we adopt a Gaussian Process (GP) as a surrogate or emulator. The response $y(\vr{x})$ to arbitrarily fixed input is modelled as the sum $f(\vr{x})+e(\vr{x})$ of two Gaussian random variables encapsulating coherent signal and incoherent noise. The latter is characterized by a zero-mean distribution that is independent of the input:
            \begin{equation*}
                e(\vr{x}) \sim \gauss{0}{\sigma^{2}_\vr{e}}
            \end{equation*}

            The signal $f(\vr{x})$ is characterized by its covariance kernel $\sigma^{2}_\vr{f} k(\vr{x}_{n},\vr{x})$ which measures the similarity between inputs $\vr{x}_{n}$ and $\vr{x}$, and propagates any similarity to $y(\vr{x}_{n})$ and $y(\vr{x})$. In the majority of applications, the kernel is naturally stationary, a function of $(\vr{x}-\vr{x}_{n})$ alone. We shall further assume that the kernel is twice differentiable at its maximum $(\vr{x}=\vr{x}_{n})$. Hence, the Hessian at the maximum must be symmetric negative semi-definite and therefore diagonalizes to
            \begin{equation*}
                \partial_{\vr{x}\vr{x}} \log k(\vr{x},\vr{x}) \deqr -\Theta^{\intercal}\Lambda^{-2}\Theta
            \end{equation*}

            When $\modulus{\vr{x}-\vr{x}_{n}}$ is large the kernel value is miniscule in any any relevant direction. The kernel details are therefore largely irrelevant to the response \emph{any} time $\modulus{\vr{x}-\vr{x}_{n}}$ is large, advocating (if not justifying)\todo{which is it?} the Taylor approximation\todo{this paragraph basically needs to say when x is big y is small so we can use this expansion, and if this is common in the literature (which I imagine it is) we include a suitable reference}
            \begin{equation*}
                k(\vr{x}_{n},\vr{x}) = 
                \exp \left(-\frac
                    {(\vr{x}-\vr{x}_{n})^{\intercal} \Theta^{\intercal}\Lambda^{-2}\Theta (\vr{x}-\vr{x}_{n})}{2}
                    \left(1+O(\modulus{\vr{x}-\vr{x}_{n}})\right)
                \right)             
            \end{equation*}
            The differentiability we have imposed forces the power spectrum of the signal $f$ to decay rapidly. 
            Modes of response oscillating rapidly with $\vr{x}$ are interpreted as noise by the GP, as the kernel smoothes $y$ into $f$. Such regularization is often, but not always, desirable, to avoid wildly unreliable interpolation of an overfit regression.

        \subsection{Kernel Optimization}
            In order to deal with the curse of dimensionality we propose to find orthogonal rotation matrix $\Theta$ and diagonal length-scale matrix $\Lambda$ which best fit observed responses $y(\vr{X}^{\intercal})$. The largest lengthscales in $\Lambda$ mark the least relevant directions that can be ignored. However, the best fit must optimize $M(M+1)/2+2$ hyperparameters simultaneously to determine $\Theta, \Lambda, \sigma^{2}_\vr{f}$ and $\sigma^{2}_\vr{e}$. As a direct optimization of such a large problem may result in obtaining only local optima. Exploratory grid search is astronomically expensive $O(\exp(M(M+1)/2)$, likewise any random sampling which is not hopelessly sparse. Perhaps for these reasons\todo{the previous two sentences need to be put after the descript of what the literature do, as a contrast to what we're going to do}, $\Theta$ has always been fixed as identity in the literature. The lengthscales comprising $\Lambda$\todo{who has done this?} are also usually identical, furnishing a radial basis function (RBF) kernel. The few studies where $\Lambda$ is not identical speak of an automatic relevance determination (ARD) kernel, with model order reduction in mind\todo{who says this?}. However, admitting no rotation $\Theta$ severely restricts the reductions available.\todo{is this sentence necessary?} If relevant inputs are mutually dependent, they must all be retained. Rotation, on the contrary, combines them into a low dimensional subspace.\todo{as we're not testing dependent variables this should be removed}

        \subsection{Global Sensitivity Analysis}
            This paper proposes to achieve kernel optimization indirectly, via global sensitivity analysis (GSA).
            The surrogate expectation
            \begin{equation*}
                \ev[\Omega]{y(\vr{x})} = \ev[\Omega]{f(\vr{x})} \deqr \bar{f}(\vr{x})
            \end{equation*}
            has a variation (over $\vr{x} \in \st{R}^{M}$) which can be apportioned by Sobol' index
            \begin{equation*}
                S_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}}) \deq \var[\vr{x}]{\ev[\vr{x}]{\bar{f}(\vr{x}) \vert (\Theta \vr{x})_{\vr{m}}}} / \var[\vr{x}]{\bar{f}(\vr{x})} \leq 1
            \end{equation*}
            to subspaces $(\Theta \vr{x})_{\vr{m}}$ of dimension $m\leq M$. These may be calculated analytically for the exponential quadratic kernel used here. To cure to the curse of dimensionality is to find $(\Theta)_{\vr{m}\times\vr{M}}$ such that $S_{\vr{m}} \approx 1$ for $m\ll M$. The rotation sub-matrix $(\Theta)_{\vr{m}\times\vr{M}}$ has a manageable number of elements if $m$ is small. This paper takes the most economical approach,maximizing $S_{\vr{m}}$ for $m=1,\ldots,M$ in turn, to find the most relevant direction, then the second most relevant, and so on. 


    \section{Methodology} \label{sec:Method}
        Let $\vr{X}$ be the $(N \times M)$ design matrix of observed inputs eliciting the \M{N} response $y(\vr{X}^{\intercal})$. The observations are standardized such that
        \begin{align*}
            (\vr{0})_\vr{M} = \ev{\vr{x}_{n}} \deq \sum_{n=1}^{N} (\vr{X})_{n \times \vr{M}}^{\intercal} 
            &\QT{;} 1 = \var{\vr{x}_{n}} = N^{-1}\sum_{n=1}^{N} (\vr{X})_{n \times \vr{M}} (\vr{X})_{n \times \vr{M}}^{\intercal}
            \\
            0 = \ev{y(\vr{x}_{n})} \deq \sum_{n=1}^{N} (y(\vr{X}^{\intercal}))_{n} 
            &\QT{;} 1 = \var{y(\vr{x}_{n})} = N^{-1} y(\vr{X}^{\intercal})^{\intercal}y(\vr{X}^{\intercal})
        \end{align*}
        where boldface subscripts refer to the multi-indices
        \begin{equation} \label{eq:Method:MultiIndexDef}
            \emptyset \deqr \vr{0} \subseteq\vr{m}\deq(1,\ldots,m) \subseteq \vr{M}
        \end{equation}
        which always precede superscript operations (such as transposition or inversion). For brevity, we shall admit vector Gaussian probability densities $p\!\left((\vr{z})_{\vr{m}} ; (\vr{Z})_{\vr{m}\times\vr{J}}, \Sigma_{\vr{z}}\right)$ such that
        \begin{multline} \label{eq:Method:pDef}
            \left(p\!\left((\vr{z})_{\vr{m}} ; (\vr{Z})_{\vr{m}\times\vr{J}}, \Sigma_{\vr{z}}\right)\right)_{j} \\
            \deq (2 \pi)^{-M/2} \modulus{\Sigma_{\vr{z}}}^{-1/2} \exp\left(-\frac
            {(\vr{z}-(\vr{Z})_{\vr{m}\times j})^{\intercal} \Sigma_{\vr{z}}^{-1} (\vr{z}-(\vr{Z})_{\vr{m}\times j})}{2}
            \right)             
        \end{multline}
        naturally collapsing to the (scalar) normal multivariate density when $J=1$.

        \subsection{Gaussian Process Surrogate} \label{sub:Method:GP}
            Non-parametric GP regression fits signal $f$ and noise $e$ Gaussian processes to
            \begin{equation} \label{eq:Method:GP:Problem}
                y(\vr{X}^{\intercal}) = f(\vr{X}^{\intercal}) + e(\vr{X}^{\intercal})
            \end{equation}
            This work exclusively employs objective Bayesian priors
            \begin{align*}
                f(\vr{X}^{\intercal}) &\sim \gauss{(\vr{0})_{\vr{N}}}{\sigma_{\vr{f}}^{2} k(\vr{X}^{\intercal},\vr{X}^{\intercal})} \\
                e(\vr{X}^{\intercal}) &\sim \gauss{(\vr{0})_{\vr{N}}}{\sigma_{\vr{e}}^{2} (\vr{I})_{\vr{N}\times\vr{N}}} 
            \end{align*}
            built on an ARD kernel \cite{Wipf.Nagarajan2007, Neal1996}
            \begin{equation} \label{eq:Method:GP:Kernel}
                k(\vr{x}_{n},\vr{x}) \deq 
                (2 \pi)^{M/2} \modulus{\Lambda} p\!\left(\vr{x} ; \vr{x}_{n}, \Lambda^2\right) 
            \end{equation}
            with diagonal positive definite lengthscale matrix \(\Lambda\).
            Bayesian conditioning ultimately furnishes the predictive process
            \begin{equation*}
                y(\vr{x}) \sim \gauss{\bar{f}(\vr{x})}{\Sigma_{\vr{f}}(\vr{x}) + \sigma_{\vr{e}}^{2}}
            \end{equation*}
            with signal mean and variance
            \begin{equation} \label{eq:Method:GP:MeanAndVariance}
                \begin{aligned}
                    \bar{f}(\vr{x}) &\deq \sigma^{2}_\vr{f} k(\vr{x},\vr{X}^{\intercal})
                    \vr{K}^{-1} y(\vr{X}^{\intercal}) \\
                    \Sigma_{\vr{f}}(\vr{x}) &\deq \sigma^{2}_\vr{f} k(\vr{x},\vr{x})
                    - \sigma^{2}_\vr{f} k(\vr{x},\vr{X}^{\intercal})
                    \vr{K}^{-1} \sigma^{2}_\vr{f} k(\vr{X}^{\intercal},\vr{x})
                \end{aligned}
            \end{equation}
        where
            \begin{equation} \label{eq:Method:GP:KDef}
                \vr{K} \deq \sigma^{2}_\vr{f} k(\vr{X}^{\intercal},\vr{X}^{\intercal}) + \sigma_{\vr{e}}^{2} (\vr{I})_{\vr{N}\times\vr{N}}       
            \end{equation}
        The $M+2$ hyperparameters constituting $\Lambda, \sigma_{\vr{f}}$ and $\sigma_{\vr{e}}$ are simultaneously optimized for maximium marginal likelihood $\mathsf{p}\lbrack y \vert \vr{X}^{\intercal}\rbrack$, using the GPy software library (refeence).

        \subsection{Global Sensitivity Analysis} \label{sub:Method:GSA}
            Imagine a sample datum \(\vr{u}\) is drawn from a standardized normal test distribution
            \begin{equation} \label{eq:Method:GSA:uDist}
                \vr{u} \sim \gauss{(\vr{0})_{\vr{M}}}{(\vr{I})_{\vr{M}\times\vr{M}}}
            \end{equation}
            The datum basis is rotated to
            \begin{equation} \label{eq:Method:GSA:Rotation}
                \vr{x} \deqr \Theta^{\intercal} \vr{u}
            \end{equation}
            eliciting the conditional surrogate responses
            \begin{equation} \label{eq:Method:GSA:fmDef}
                f_{\vr{m}}((\vr{u})_{\vr{m}}) 
                    \deq \ev{\bar{f}(\Theta^{\intercal} \vr{u}) \vert (\vr{u})_{\vr{m}}}
            \end{equation}    
            Knowledge of $\vr{u}$ herein ranges from totally conditional $f_{\vr{M}}(\vr{u})=\bar{f}(\vr{x})$ to unconditional ignorance $f_{\vr{0}}=\ev{\bar{f}(\vr{x})}$.
            \Crefrange{eq:Method:GP:Kernel}{eq:Method:GSA:uDist} enable analytic integration yielding
            \begin{equation} \label{eq:Method:GSA:fmCalc}
                f_{\vr{m}}((\vr{u})_{\vr{m}}) = \tilde{\vr{f}}^{\intercal} \; 
				\frac 
					{p\!\left((\vr{u})_{\vr{m}} ; (\vr{T})_{\vr{N}\times\vr{m}}^{\intercal}, (\Sigma)_{\vr{m}\times\vr{m}}\right)}
					{p\!\left((\vr{u})_{\vr{m}} ; (\vr{0})_{\vr{m}},(\vr{I})_{\vr{m}\times\vr{m}}\right)}
            \end{equation}
            where $\tilde{\vr{f}}$ is the Hadamard (element-wise) product $\circ$ of two vectors
            \begin{equation} \label{eq:Method:GSA:gDef}
                \tilde{\vr{f}} \deq
                (2 \pi)^{M/2} \modulus{\Lambda} p\!\left(\vr{0};\vr{X}^{\intercal} , \Lambda^{2} + \vr{I}\right)
                \circ\left(\sigma^{2}_\vr{f} \vr{K}^{-1} y(\vr{X}^{\intercal})\right) 
            \end{equation}
            and
            \begin{align} \label{eq:Method:GSA:TSigmaDef}
                \vr{T} &\deq 
                    \vr{X} \left(\Lambda^{2} + \vr{I}\right)^{-1} \Theta^{\intercal} \\
                \Sigma &\deq 
                    \Theta \left(\Lambda^{-2} + \vr{I}\right)^{-1} \Theta^{\intercal}
            \end{align}
            According to these formulae, the unconditional surrogate response is
            \begin{equation} \label{eq:Method:GSA:f_0}
                f_{\vr{0}} = \ev{\bar{f}(\vr{x})} = \tilde{\vr{f}}^{\intercal} (\vr{1})_{\vr{N}}
            \end{equation}
            which does not depend on $\Theta$ of course. Standardization of $y(\vr{X}^{\intercal})$ instills an expectation of precisely zero here if $\vr{x}_{n} \sim \gauss{(\vr{0})_{\vr{M}}}{(\vr{I})_{\vr{M}\times\vr{M}}}$ (which is often not exactly true).

            Conditional variances may now be calculated as
            \begin{equation} \label{eq:Method:GSA:DmDef}
                D_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}}) \deq \var{f_{\vr{m}}((\vr{u})_{\vr{m}})}
                = \frac {\tilde{\vr{f}}^{\intercal} \; \vr{W}_{\vr{m}} \; \tilde{\vr{f}}}
                    {\modulus{2(\Sigma)_{\vr{m}\times\vr{m}} - (\Sigma)_{\vr{m}\times\vr{m}}^{2}}^{1/2}}
                - f_{\vr{0}}^{2}
            \end{equation}
            where
            \begin{equation} \label{eq:Method:GSA:WmDef}
                \begin{aligned}
                    &(\vr{W}_{\vr{m}})_{n \times o} \deq
                    \exp\!\left(\frac{
                        -(\vr{T})_{n\times\vr{m}}(\Sigma)_{\vr{m}\times\vr{m}}^{-1}
                        (\vr{T})_{n\times\vr{m}}^{\intercal} 
                        -(\vr{T})_{o\times\vr{m}}(\Sigma)_{\vr{m}\times\vr{m}}^{-1}
                        (\vr{T})_{o\times\vr{m}}^{\intercal}
                        }{2}\right) \\
                        &\times\exp\!\left(\frac{
                            + \left((\vr{T})_{n\times\vr{m}}+(\vr{T})_{o\times\vr{m}}\right)
                            (\Psi)_{\vr{m}\times\vr{m}}^{-1}(\Sigma)_{\vr{m}\times\vr{m}}^{-1}
                            \left((\vr{T})_{n\times\vr{m}}^{\intercal}+(\vr{T})_{o\times\vr{m}}^{\intercal}\right)
                            }{2}\right)
                \end{aligned}
            \end{equation}
            and
            \begin{equation} \label{eq:Method:GSA:PhiDef}
                \Psi \deq \Theta \left(\Lambda^{-2}+\vr{I}\right)^{-1}\left(2\Lambda^{-2}+\vr{I}\right) 
                \Theta^{\intercal}
            \end{equation}
            The proportion of response variance ascribable to the first $m$ basis directions of $\vr{u}$ is given by the Sobol' index
            \begin{equation} \label{eq:Method:GSA:SDef}
                S_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}}) \deq D_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}})/D_{\vr{M}}(\Theta) \leq S_{\vr{M}}(\Theta) = 1
            \end{equation}
            Analytic expressions for $\partial_{\Theta} D_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}})$ have been obtained from \cref{eq:Method:GSA:DmDef} using standard formulae for differentating matrix inverses and determinants. As $D_{\vr{m}}$ projects $M$-dimensional $(\vr{x})_{\vr{M}}$ onto $m$-dimensional $(\vr{u})_{\vr{M}}$, the result is affected by just a few components of rotation:
            \begin{equation} \label{eq:Method:GSA:partialD}
                \left(\partial_{\Theta} D_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}})\right)_{i \times j} \neq 0 \quad \Longrightarrow \quad i \leq m < M
            \end{equation}
            In particular $D_{\vr{M}}(\Theta)=D_{\vr{M}}$ and $S_{\vr{M}}(\Theta)=1$ are independent of $\Theta$, as there is no projection, only rotation, in transforming $(\vr{x})_{\vr{M}}$ into $(\vr{u})_{\vr{M}}$.

        \subsection{Basis Optimization} \label{sub:Method:BO}
            At this point in the analysis, everything has been fixed save the rotation
            \begin{equation} \label{eq:Method:BO:Rotation}
                \vr{u} \deq \Theta \vr{x}
            \end{equation}
            relating sampling distribution $\vr{u} \sim \gauss{(\vr{0})_{\vr{M}}}{(\vr{I})_{\vr{M}\times\vr{M}}}$ to the input of the surrogate response $\bar{f}(\vr{x})$. This rotation will now be determined by maximizing the relevance -- as measured by Sobol' index -- of each $\vr{u}$-direction in turn. This means optimizing $\Theta$ in \cref{eq:Method:BO:Rotation} row by row from top to bottom. 
            
            Row orthonormality leaves just $(M-m-1)$ elements free in row $m$, which we encode as
            \begin{equation} \label{eq:Method:BO:XiDef}
                (\Theta)_{\vr{m}\times\vr{M}} \deqr (\Xi)_{\vr{m}\times\vr{M}} \tilde{\Theta}
            \end{equation}
            where \(\Xi\) is orthogonal, and identical on the $(m-1)$ rows already optimized
            \begin{equation} \label{eq:Method:BO:XiConstraints}
                \begin{aligned}
                    (\Xi)_{\vr{m}\setminus\set{m}\times\vr{M}} &= \vr{I}_{\vr{m}\setminus\set{m}\times\vr{M}} \\
                    (\Xi)_{m\times\vr{m}\setminus\set{m}} &= (\vr{0})_{1\times\vr{m}\setminus\set{m}} \\
                    (\Xi)_{m\times m} &= \left(1 - \sum_{k=m+1}^{M} (\Xi)_{m\times k}^{2} \right)^{1/2}
                \end{aligned}            
            \end{equation}
            The last line induces a derivative adjustment
            \begin{equation} \label{eq:Method:BO:derivAdjust}
                \frac{\partial}{\partial\,(\Xi)_{m\times k}} = \frac{\partial}{\partial\,(\Xi)_{m\times k}} - 
                \frac{(\Xi)_{m \times k}}{(\Xi)_{m \times m}}\frac{\partial}{\partial\,(\Xi)_{m\times m}}
            \end{equation}
            which should be exploited by the optimizer as a powerful repellant to orthonormality violations.
            This work uses a BFGS optimizer, fed an analytic Jacobian.

            Given these constraints, row $m$ is optimally determined by
            \begin{equation} \label{eq:Method:BO:OptimalRow}
                (\Xi)_{m\times\vr{M}\setminus\vr{m}} = \argmax_{(\Xi)_{m\times\vr{M}\setminus\vr{m}}} S_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}}) = \argmax_{(\Xi)_{m\times\vr{M}\setminus\vr{m}}} D_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}})
            \end{equation}
            The optimal row $m$ is then incorporated in $\tilde{\Theta}$ and $\Xi$ according to
            \begin{equation} \label{eq:Method:BO:incorpUpadate}
                \begin{gathered}
                    \tilde{\Theta} = \vr{Q}^{\intercal} \  \text{where } \Theta^{\intercal} = \vr{Q}\vr{R} \text{ is the QR factorization of the update} \\
                    (\Xi)_{\vr{m}\times\vr{M}} = \vr{I}_{\vr{m}\times\vr{M}}
                \end{gathered}
            \end{equation}        
            ready to optimize row $m+1$. 
            Optimization followed by incorporation is performed for $m=1,\ldots, M-1$ in turn to entirely optimize $\Theta$.
            The later rows could be left unoptimized, though they are successively cheaper to obtain.
    
        \subsection{Summary} \label{sub:Method:Summary}
            The main loop of the algorithm is described i:
            
            \begin{algorithm}
            \caption{Summary of the basis optimization algorithm.}
                \begin{algorithmic}[1]
                    \REPEAT
                        \STATE Fit GP surrogate to $y(\vr{X}^{\intercal})$, determining $\bar{f}(\vr{x})$ according to \cref{sub:Method:GP}
                        \STATE Set $\tilde{\Theta} \leftarrow \Theta \leftarrow \Theta_{\Pi} \leftarrow \vr{I}$
                        \FOR{$m=1$ \TO $M$}
                            \STATE According to \cref{sub:Method:GSA}, optimize \label{bum}
                            $$(\Xi)_{m\times\vr{M}\setminus\vr{m}} \leftarrow \argmax_{(\Xi)_{m\times\vr{M}\setminus\vr{m}}} D_{\vr{m}}((\Theta)_{\vr{m}\times\vr{M}})$$
                            where $(\Theta)_{\vr{m}\times\vr{M}} \deqr (\Xi)_{\vr{m}\times\vr{M}} \tilde{\Theta}$
                            \STATE  Update $\tilde{\Theta} \leftarrow \vr{Q}^{\intercal}$ where $\Theta^{\intercal} = \vr{Q}\vr{R}$
                        \ENDFOR
                        \STATE Update the input basis to $\vr{X}^{\intercal} \leftarrow \Theta \vr{X}^{\intercal}$
                        \STATE Update the overall rotation to $\Theta_{\Pi} \leftarrow \Theta \Theta_{\Pi}$
                    \UNTIL{$\Theta \approx \vr{I}$}
                \end{algorithmic}
            \end{algorithm}

            During testing the optimization in Step \ref{bum} is found to be prone to converge to local optima, especially in the first iteration or two of the outermost loop. An approach was therefore developed that the early iterations explore the behaviour of \M{(\Xi)_{m\times\vr{M}\setminus\vr{m}}} by grid or randomized search, before attempting exploitation by gradient descent).

            As the input basis is updated at each step, \M{\vr{X} = \vr{U}} ultimately. 
            The key output of the algorithm is the overall rotation \M{\Theta_{\Pi}} of the original basis for \vr{x} to the optimal basis for \vr{u}.
            
    \section{Results} \label{sec:Results}
        In this section, the method described in \cref{sub:Method:Summary} is applied to a series of test functions to evaluate its performance. , using $N \in {100, 200, 400, 800, 1600}$ data of $M=5$ input dimensions. All quoted results are the mean over two folds (each with $N$ training data and $N$ test data).
        
        
         using $N \in {100, 200, 400, 800, 1600}$ data of $M=5$ input dimensions. All quoted results are the mean over two folds (each with $N$ training data and $N$ test data).
        In each case an \M{N \times M} design matrix \vr{X} is sampled from a standard normal distribution. 
        The input to the test function \M{f\colon \lbrack x_-, x_+ \rbrack^{M} \to \st{R}} is generally constructed as
        \begin{equation} \label{def:Xhat}
            \vr{\hat{X}}^{\intercal} = (x_+ - x_-) c(\Phi \vr{X}^{\intercal}) + x_-(\vr{1})_{\vr{M} \times \vr{N}}
        \end{equation}
        where \M{c\colon \st{R}^M\to\st{R}^M} is the cumulative density function for \M{M} independent standard normal random variables, and \M{\Phi} is a test rotation matrix. 
        The corresponding optimal input rotation from \cref{sub:Method:Summary} is
        \begin{equation}
            \Theta_{\Pi} = \begin{cases}
                \Theta_{\vr{1}} & \QT{if \M{\Phi} is identity matrix \M{\vr{1}}} \\
                \Theta_{\vr{R}} & \QT{if \M{\Phi} is a random rotation matrix \M{\Phi_{\vr{R}}}}
            \end{cases}
        \end{equation}
        which should recover the random rotation as
        \begin{equation}
            \Theta_{\vr{R}} \cong \Theta_{\vr{1}} \Phi_{\vr{R}}
        \end{equation}
        However, this is congruence, not equality.
        For each test function, the intial GP fit is assessed by test statistics from independent data (from the other fold), together with errors in the calculated Sobol' indices. The latter are important as they at the heart of subsequent calculations. The input basis is then optimized, calculating $\Theta_{\vr{1}}$.
        A reduced dimensionality \M{\underline{M}} for the optimized basis is determined as 
        \begin{equation} \label{eq:Results:Mbar}
            \min \setbuilder{\underline{M} \leq M}{S_{\underline{\vr{M}}} \geq 0.90}
        \end{equation}
        A GP is fit to this reduced input, and its test statistics compared with the initial GP.

        The whole procedure is then repeated (with entirely fresh data) to which a random input rotation $\Phi_{\vr{R}}$ is applied. The input basis is optimized, calculating $\Theta_{\vr{R}}$, whereas the reduced dimensionality \M{\underline{M}} is not re-assessed, but retained from the unrotated analysis.

        Finally the rotated and unrotated versions are compared for congruence, essentially we wish


        \begin{equation}
            \modulus{\epsilon_{\Theta}} = \norm{\left(\Phi_{\vr{R}} \Theta_{\vr{1}} \Theta_{\vr{R}}^{\intercal} - \vr{1}\right)_{\underline{\vr{M}}\times\underline{\vr{M}}}} / \underline{M}
        \end{equation}
        where \M{\norm{\cdot}} is the usual Frobenius matrix norm and \M{\underline{\vr{M}}} includes no irrelevant input directions (which might be rotated versions of each other). 
        All inputs and outputs are standardized to a mean of zero and variance of 1. All functions are tested with and without output noise \M{e \sim \gauss{0}{0.001}} added to \M{f(\vr{\hat{X}}^{\intercal})}. 
        In the noiseless cases \M{e=0} we floor the Gaussian process noise variance as \M{\epsilon_\vr{e} \geq 10^{-6}} in order to prevent numerical instability in the calculation of Sobol' indices which is otherwise observed.

        \subsection{Sine Function} \label{sub:Results:Sin}
            \begin{equation} \label{def:Sin}
                f(\vr{\hat{x}}) \deq \sin(\vr{\hat{x}}_1)
            \end{equation}
            \begin{gather*}
                \lbrack x_-, x_+ \rbrack \deq \lbrack -\pi, +\pi \rbrack \\
                S_{\vr{1}} \deq S_{(1)} = 1
            \end{gather*}

%            \csvautotabular{results/sin.u1.rom.5/ard.formatted.S.csv}

 %           \csvautotabular{results/sin.u1.rom.5/rom.optimized.formatted.S.csv}

            Fitting an initial GP recovers the exact Sobol' indices to within an accuracy of 0.005 (precision actually decreasing with the number of data). Optimizing the input basis improves this to $10^{-8}$, which can only be due to repeating GP regression as 4 iterations were run to optimize $\Theta$, even though convergence is immediate.
            Applying a rand[h]om rotation \M{\Phi_{\vr{R}}}, the exact Sobol' indices are recovered to within $10^{-8}$ after 3 iterations. The 1D active subspace measures are recorded in 

            \begin{table}[h]
                \centering
                \csvreader[tabular=|r|r|r|, 
                table head=\hline Noise & $N$ & $\modulus{\vr{\underline{u}}_{1}}$ \\\hline, 
                table foot=\hline]
                {results/sin.1.random.5/rom.optimized.formatted.Theta_Analyzed.csv}
                {}{\csvcoli & \csvcolii & \csvcoliii}                
            \label{tab:Results:SinTheta} \end{table}
%            \csvreader[tabular=|r|r|r|,
%            table head=\hline & Person & Matr.~No.\\\hline\hline,
%            {grade.csv}{1=\name,2=\firstname,3=\matnumber}%
%            {\thecsvrow & \firstname~\name & \matnumber}%
            \csvautotabular{results/sin.1.random.5/rom.optimized.formatted.Theta_Analyzed.csv}


        \subsection{Decoupled Ishigami Function} \label{sub:Results:Ishigami}
            \begin{equation} \label{def:Ishigami}
                f(\vr{x}) \deq \left(1 + b \vr{x}_3^4\right) \sin(\vr{x}_1) + a \sin^{2}(\vr{x}_2)
            \end{equation}
            \begin{gather*}
                a = 2.0 \QT{;} b = 0 \\
                S_{\vr{1}} = 0.5 \QT{;}S_{\vr{2}} = 1
            \end{gather*}

%            \csvautotabular{results/ishigami.rom.5/ard.formatted.S.csv}
            
%            \csvautotabular{results/ishigami.rom.5/rom.optimized.formatted.S.csv}

%            \csvautotabular{results/ishigami.random.5/rom.optimized.formatted.S.csv}

        \subsection{Ishigami Function} \label{sub:Results:Ishigami}
            \begin{equation} \label{def:Ishigami}
                f(\vr{x}) \deq \left(1 + b \vr{x}_3^4\right) \sin(\vr{x}_1) + a \sin^{2}(\vr{x}_2)
            \end{equation}
            \begin{gather*}
                a = 7.0 \QT{;} b = 0.1 \\
                S_{\vr{1}} = 0.3139 \QT{;}S_{\vr{2}} = 0.7563 \QT{;} S_{\vr{3}} = 1
            \end{gather*}

%            \csvautotabular{results/ishigami.rom.5/ard.formatted.S.csv}
            
%            \csvautotabular{results/ishigami.rom.5/rom.optimized.formatted.S.csv}

%            \csvautotabular{results/ishigami.random.5/rom.optimized.formatted.S.csv}

        \subsection{Sobol' G Function} \label{sub:Results:SobolG}
            \begin{equation} \label{def:SobolG}
                f(\vr{x}) \deq \prod_{i=1}^{D}{\frac{\modulus{4\vr{x}_i - 2} + \vr{a}_{i}}{1+\vr{a}_{i}}}
            \end{equation}
            \begin{gather*}
                \vr{a}_{i} = (i-1)/2 \\
                S_{\vr{1}} = 0.4107 \QT{;}S_{\vr{2}} = 0.6541 \QT{;} S_{\vr{3}} = 0.8113 \QT{;} S_{\vr{4}} = 0.9203 \QT{;} S_{\vr{5}} = 1
            \end{gather*}
 %           {{"si[1]", 0.410719}, {"si[2]", 0.182542}, {"si[3]", 0.10268}, {"si[4]", 0.065715}, {"si[5]", 0.0456354}}
 %           {{"sc[1]", 0.410719}, {"sc[2]", 0.654107}, {"sc[3]", 0.811296}, {"sc[4]", 0.92028}, {"sc[5]", 1.}}

 %           \csvautotabular{results/sobol_g.rom.5/ard.formatted.S.csv}
            
%            \csvautotabular{results/sobol_g.rom.5/rom.optimized.formatted.S.csv}

%            \csvautotabular{results/sobol_g.random.5/rom.optimized.formatted.S.csv}


     \section{Conclusion} \label{sec:Conclusion}


        %% The Appendices part is started with the command \appendix;
        %% appendix sections are then done as normal sections
        %% \appendix

        %% \section{}
        %% \label{}

        %% If you have bibdatabase file and want bibtex to generate the
        %% bibitems, please use
        %%

        %% else use the following coding to input the bibitems directly in the
        %% TeX file.


        \bibliographystyle{elsarticle-num} 
    \bibliography{main}

\end{document}

\endinput
